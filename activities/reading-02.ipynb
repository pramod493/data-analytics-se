{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 2: Basics of Probability Theory\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To understand the interprtation of probability theory as a representation of our state of knowledge.\n",
    "+ To learn about the common sense assumptions that give rise to the basic probability rules.\n",
    "+ To gain experience with the basic probability rules.\n",
    "\n",
    "## References\n",
    "\n",
    "+ [Chapters 1 and 2, Probability Theory: The Logic of Science, E. T. Jaynes](https://bayes.wustl.edu/etj/prob/book.pdf).\n",
    "\n",
    "## Probability as a representation of our state of knowledge\n",
    "\n",
    "Let's call $I$ all the *information* you have a this given moment.\n",
    "And I am talking about absolutely everything: what your parents taught you, what you learned in school, what you learned in college, what your eyes see right now on some scientific instruments.\n",
    "Now consider a, well-defined, sentence $A$ that says something about the world.\n",
    "For example, $A$ could be \"The result of the next coin toss John performs will be heads.\"\n",
    "Or anything really.\n",
    "We want a technical machinery that can turn all the information $I$ we have into a real number that tells us how plausible it is that $A$ is true.\n",
    "This is what probability theory does.\n",
    "It gives us such a number.\n",
    "Call it $p(A|I)$ and read it as \"the probability that $A$ is true given that we know $I$.\"\n",
    "So, probability theory is an attempt to represent our state of knowledge about the world.\n",
    "\n",
    "## But what about frequencies?\n",
    "\n",
    "In introductory courses to probability or statistics, we usually learn that the probability of an event is the frequency with each it occurs in nature.\n",
    "This is absolutely fine if the event is something that indeed occurs repeatedly.\n",
    "However, this intrepretation is quite restrictive.\n",
    "In particular, what can we say about an event that can happen only once?\n",
    "This interpretation forbids the quantification of epistemic uncertainties.\n",
    "We will expand the interpretation of probability.\n",
    "It can be shown, see E. T. Jaynes book for the proof, that this interpretation is compatible with the frequency interpretation.\n",
    "That is, when events occur repeatedly then the probabilities do become frequencies.\n",
    "\n",
    "## The common sense assumptions that give rise to the basic probability rules.\n",
    "\n",
    "> Probability theory is nothing but common sense reduced to calculation. Pierre-Simon Laplace, Théorie analytique des probabilités (1814)\n",
    "\n",
    "Consider the following three ingedients:\n",
    "+ A: a logical sentence\n",
    "+ B: another logical sentence\n",
    "+ I: all the information we know\n",
    "\n",
    "No other restriction apart that A and B are not contradictions.\n",
    "\n",
    "We need a bit of notation so that we write less math:\n",
    "\n",
    "+ $\\text{not}\\;A \\equiv \\neg A$\n",
    "+ $A\\;\\text{and}\\;B \\equiv A, B \\equiv AB$\n",
    "+ $A\\;\\text{or}\\;B \\equiv A+B$\n",
    "\n",
    "Now, let's try to make a robot that can argue under uncertainty.\n",
    "It should be able to take logical sentences (such as $A$ and $B$ above) and argue about them using all the information it has.\n",
    "What sort of system should govern this robot.\n",
    "The following desiderata seem reasonable:\n",
    "\n",
    "+ Degrees of plausibility are represented by real numbers.\n",
    "+ The system should have a qualitative correspondence to common sense.\n",
    "+ The system should be consistence in the sense that:\n",
    "   - If a conclusion can be reached in two ways, each way must lead to the same result.\n",
    "   - All evidence relevant to a question should be taken into account.\n",
    "   - Equivalent states of knowledge must be represented by equivalent plausibility assignments.\n",
    "   \n",
    "[Cox's theorem](www.sciencedirect.com/science/article/pii/S0888613X03000513?via=ihub) shows that:\n",
    "\n",
    "> The desiderata are enough derive the rules of probability theory.\n",
    "\n",
    "### Talking about probabilities\n",
    "\n",
    "We read $p(A|BI)$ as:\n",
    "\n",
    "+ the probability of A being true given that we know that B and I are true; or\n",
    "+ the probability of A being true given that we know that B is true; or\n",
    "+ the probability of A given B.\n",
    "\n",
    "### Interpratation of probabilities\n",
    "\n",
    "The probability $p(A|BI)$ is a number between 0 and 1 quantifying the degree of plausibility that A is true given B and I.\n",
    "Specifically:\n",
    "\n",
    "+ $p(A|B,I) = 1$ when we are certain that A is true if B is true (and I).\n",
    "+ $p(A|B,I) = 0$ when we are certain that A is false if B is true (and I).\n",
    "+ $0< p(A|B,I) < 1$ when we are uncertain about A if B is true (and I).\n",
    "+ $p(A|B,I) = \\frac{1}{2}$ when we are completely ignorant about A if B is true (and I).\n",
    "\n",
    "## The rules of probability\n",
    "\n",
    "There are two rules of probability from which everything else can be derived.\n",
    "These are direct consequencies of the desiderate and Cox's theorem.\n",
    "They are:\n",
    "\n",
    "+ The **obvious rule**:\n",
    "$$\n",
    "p(A|I) + p(\\neg A|I) = 1.\n",
    "$$\n",
    "The sum rule is obvious. It states that either $A$ or its negation $\\neg A$ must be true.\n",
    "(It is vitally important that you do not try to apply probability in a system that includes contradictions.)\n",
    "\n",
    "+ The **product rule** (or Bayes' rule or Bayes' theorem):\n",
    "$$\n",
    "p(A,B|I) = p(A|B,I)p(B|I).\n",
    "$$\n",
    "The product rule is not obvious.\n",
    "Understanding it requires a bit of meditation.\n",
    "It states that the probability of A and B is the probability of A given that B is true times the probability that B is true.\n",
    "Even though the correspondance is not one to one, visualizing events using the Venn diagrams helps in understanding the product rule:\n",
    "![Venn diagram](venn.png)\n",
    "In this diagram:\n",
    "    - $p(A,B|I)$ corresponds to the brown area (normalized by the area of I).\n",
    "    - $p(B|I)$ is the area of $B$ (normalized by the area of I).\n",
    "    - $p(A|BI)$ is the brown area (normalized by the area of B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Drawing balls from a box without replacement (1/3)\n",
    "\n",
    "Consider the following information I:\n",
    "\n",
    "> We are given a box with 10 balls 6 of which are red and 4 of which are blue. The box is sufficiently mixed so that when we get a ball from it, we don't know which one we pick. When we take a ball out of the box, we do not put it back.\n",
    "\n",
    "![Urn](urn.png)\n",
    "\n",
    "Now, let's draw the first ball.\n",
    "Here is the graphical causal model up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "gu1 = Digraph('Urn1')\n",
    "gu1.node('reds', label='# red balls', style='filled')\n",
    "gu1.node('blues', label='# blue balls', style='filled')\n",
    "gu1.node('first', label='1st draw')\n",
    "gu1.edge('reds', 'first')\n",
    "gu1.edge('blues', 'first')\n",
    "gu1.render('urn1_graph', format='png')\n",
    "gu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say that we draw the first ball.\n",
    "Let B_1 be the sentence:\n",
    "\n",
    "> The first ball we draw is blue.\n",
    "\n",
    "What is the probability of $B_1$?\n",
    "Our intuition tells us to set:\n",
    "$$\n",
    "p(B_1|I) = \\frac{4}{10} = \\frac{2}{5}.\n",
    "$$\n",
    "This is known as the *principle of insufficient reason*.\n",
    "We can now use the **obvious rule** to find the probability of drawing a red ball, i.e., of $\\neg B_1$.\n",
    "Of course, $\\neg B_1$ is just the sentence:\n",
    "\n",
    "> The first ball we draw is red.\n",
    "\n",
    "So, let's call it also $R_1$.\n",
    "It is:\n",
    "$$\n",
    "p(R_1|I) = p(\\neg B_1|I) = 1 - p(B_1|I) = 1 - \\frac{2}{5} = \\frac{3}{5}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the graphical model representation after we observe the first draw?\n",
    "We need to fill the node corresponding to the first draw with color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu3 = Digraph('Urn3')\n",
    "gu3.node('reds', label='# red balls', style='filled')\n",
    "gu3.node('blues', label='# blue balls', style='filled')\n",
    "gu3.node('first', label='1st draw', style='filled')\n",
    "gu3.node('second', label='2nd draw')\n",
    "gu3.edge('reds', 'first')\n",
    "gu3.edge('blues', 'first')\n",
    "gu3.edge('first', 'second')\n",
    "gu3.edge('reds', 'second')\n",
    "gu3.edge('blues', 'second')\n",
    "gu3.render('urn3_graph', format='png')\n",
    "gu3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the sentence $R_2$:\n",
    "\n",
    "> The second ball we draw is red.\n",
    "\n",
    "What is the probability of $R_2$ given that $B_1$ is true?\n",
    "We just need to use common sense to find this probability:\n",
    "+ We had 10 balls, 6 red and 4 blue.\n",
    "+ Since $B_1$ is true (the first ball was blue), we now have 6 red and 3 blue balls.\n",
    "+ Therefore, the probability that we draw a red ball next is:\n",
    "$$\n",
    "p(R_2|B_1,I) = \\frac{6}{9} = \\frac{2}{3}.\n",
    "$$\n",
    "\n",
    "Similarly, we can find the probability that we draw a red ball in the second draw given that we drew a red ball in the first draw:\n",
    "+ We had 10 balls, 6 red and 4 blue.\n",
    "+ Since $R_1$ is true (the first ball is red), we now have 5 red and 4 blue balls.\n",
    "+ Therefore, the probability that we draw a red ball next is:\n",
    "$$\n",
    "p(R_2|R_1,I) = \\frac{5}{9}.\n",
    "$$\n",
    "\n",
    "Let's consider a second draw without observing the result of the first draw.\n",
    "What is the graphical causal model now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu2 = Digraph('Urn2')\n",
    "gu2.node('reds', label='# red balls', style='filled')\n",
    "gu2.node('blues', label='# blue balls', style='filled')\n",
    "gu2.node('first', label='1st draw')\n",
    "gu2.node('second', label='2nd draw')\n",
    "gu2.edge('reds', 'first')\n",
    "gu2.edge('blues', 'first')\n",
    "gu2.edge('first', 'second')\n",
    "gu2.edge('reds', 'second')\n",
    "gu2.edge('blues', 'second')\n",
    "gu2.render('urn2_graph', format='png')\n",
    "gu2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the probability that we draw a blue ball in the first draw (A) and a red ball in the second draw (B).\n",
    "We have to use the **product rule**:\n",
    "$$\n",
    "p(B_1, R_2|I) = p(R_2|B_1,I) p(B_1|I) = \\frac{2}{3}\\frac{2}{5} = \\frac{4}{15}.\n",
    "$$\n",
    "\n",
    "### Other rules of probability theory\n",
    "\n",
    "All other rules of probability theory can be derived from the two basic rules.\n",
    "Here are some examples.\n",
    "\n",
    "#### Extention of the obvious rule\n",
    "For any two logical sentences $A$ and $B$ we have:\n",
    "$$\n",
    "p(A + B|I) = p(A|I) + p(B|I) - p(AB|I).\n",
    "$$\n",
    "In words: the probability of A or B is the probability that A is true plus that probability that B is true minus the probability that both A and B are true.\n",
    "This is very easy to understand intuitively by looking at the Venn diagram:\n",
    "![Venn diagram](venn.png)\n",
    "The probability $p(A+B|I)$ is the area of the uninion of A with B (normalized by I).\n",
    "This area is indeed the area of A (normalized by I) plus the area of B (normalized by I) minus the area of A and B (normalized by I) which was doublecounted.\n",
    "\n",
    "Let's see a formal proof of this.\n",
    "$$\n",
    "\\begin{split}\n",
    "p(A+B|I) &=& 1 - p(\\neg (A+B)|I)\\\\\n",
    "&=& 1 - p(\\neg A, \\neg B|I)\\;\\text{(obvious rule)}\\\\\n",
    "&=& 1 - p(\\neg A|\\neg B, I)p(\\neg B|I)\\;\\text{(product rule)}\\\\\n",
    "&=& 1 - \\left[1 - p(A|\\neg B, I)\\right]p(\\neg B|I)\\;\\text{(obvious rule)}\\\\\n",
    "&=& 1 - p(\\neg B|I) + p(A|\\neg B, I)p(\\neg B|I)\\\\\n",
    "&=& 1 - p(\\neg B|I) + p(A\\neg B|I)\\;\\text{(product rule)}\\\\\n",
    "&=& 1 - p(\\neg B|I) + p(\\neg B|A,I) p(A|I)\\;\\text{(product rule)}\\\\\n",
    "&=& 1 - p(\\neg B|I) + \\left[1 - p(B|A,I)\\right]p(A|I)\\;\\text{(obvious rule)}\\\\\n",
    "&=& 1 - p(\\neg B|I) + p(A|I) - p(B|A,I)p(A|I)\\\\\n",
    "&=& 1 - \\left[1 - p(B|I)\\right] + p(A|I) - p(B|A,I)p(A|I)\\;\\text{obvious rule})\\\\\n",
    "&=& p(A|I) + p(B|I) - p(B|A,I)p(A|I)\\\\\n",
    "&=& p(A|I) + p(B|I) - p(AB|I)\\;\\text{(product rule)}.\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sum rule\n",
    "\n",
    "This is the final rule we are going to consider in this lecture.\n",
    "It is one of the most important rules.\n",
    "**You absolutely have to memorize it.**\n",
    "It goes as follows.\n",
    "\n",
    "Consider the sequence of logical sentences $B_1,\\dots,B_n$ such that:\n",
    "+ One of them is definitely true:\n",
    "$$\n",
    "p(B_1 + \\dots + B_n|I) = 1.\n",
    "$$\n",
    "+ They are mutually exclusive:\n",
    "$$\n",
    "p(B_iB_j|I) = \\delta_{ij} = \\begin{cases}1,&\\;\\text{if}\\;i=j,\\\\ 0,&\\;\\text{otherwise}.\\end{cases}\n",
    "$$\n",
    "Then, for any logical sentence $A$ we have:\n",
    "$$\n",
    "p(A|I) = \\sum_{i=1}^n p(AB_i|I) = \\sum_{i=1}^n p(A|B_i,I)p(B_i|I).\n",
    "$$\n",
    "\n",
    "Again, this requires a bit of meditation.\n",
    "You take any logical sentence A and set of exclusive but exhaustive possibilities $B_1,\\dots,B_n$ and you break down the probability of $A$ in terms of the probabilities of the $B_i$'s.\n",
    "The Venn diagrams helps to understand the situation:\n",
    "![Venn sum rule](venn_sum_rule.png)\n",
    "\n",
    "The sum rule can be trivially proved by induction using only the obvious rule and the product rule.\n",
    "It is instructive to go through the proof.\n",
    "For $n=2$ we have:\n",
    "$$\n",
    "\\begin{split}\n",
    "p(A|I) &=& p(A\\;\\text{and}\\;(B_1\\;\\text{or}\\;B_2)|I)\\\\\n",
    "&=& p\\left((A\\;\\text{and}\\;B_1)\\;\\text{or}\\;(A\\;\\text{and}\\;B_2)|I\\right)\\\\\n",
    "&=& p(A\\;\\text{and}\\;B_1|I) + p(A\\;\\text{and}\\;B_2|I) - p\\left((A\\;\\text{and}\\;B_1)\\;\\text{and}\\;(A\\;\\text{and}\\;B_2)|I\\right)\\\\\n",
    "&=& p(AB_1|I) + p(AB_2|I) - p(AB_1B_2|I)\\\\\n",
    "&=& p(AB_1|I) + p(AB_2|I),\n",
    "\\end{split}\n",
    "$$\n",
    "because \n",
    "$$\n",
    "p(AB_1B_2|I) = p(B_1B_2|I)p(A|I) \\le p(B_1B_2|I) = 0.\n",
    "$$\n",
    "And then, assume that it holds for $n$, you can easily show that it also holds for $n+1$ completing the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Drawing balls from a box without replacement (2/3)\n",
    "\n",
    "Let us consider the probability of getting a red ball in the second draw without observing in the first draw $p(B_1|I)$.\n",
    "We have two possibilities for the first draw.\n",
    "We either got a blue ball (B_1 is true) or we got a red ball (R_1 is true).\n",
    "In other words $B_1$ and $R_1$ cover all possibilities and are mutually exclusive.\n",
    "We can use the sum rule:\n",
    "$$\n",
    "\\begin{split}\n",
    "p(R_2|I) &=& p(R_2|B_1,I)p(B_1|I) + p(R_2|R_1,I)p(R_1|I)\\\\\n",
    "&=& \\frac{2}{3}\\frac{2}{5} + \\frac{5}{9}\\frac{3}{5}\\\\\n",
    "&=& 0.6.\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Drawing balls from a box without replacement (3/3)\n",
    "\n",
    "If you paid close attention, in all our examples the conditioning we did followed the causal links.\n",
    "For instance, in the urn example we where writing $p(R_2|B_1,I)$ for the probability of getting a red ball in the second draw after having observed the blue ball in the first draw.\n",
    "This is the uncertainty propagation problem.\n",
    "However, conditioning on stuff **does not have to follow the causal links**.\n",
    "It is completely legitimate to ask what is the probability of a blue ball in the first draw given that you have observed that the result of the second draw is a red ball.\n",
    "The situation is visualized in the following graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gu4 = Digraph('Urn4')\n",
    "gu4.node('reds', label='# red balls', style='filled')\n",
    "gu4.node('blues', label='# blue balls', style='filled')\n",
    "gu4.node('first', label='1st draw')\n",
    "gu4.node('second', label='2nd draw', style='filled')\n",
    "gu4.edge('reds', 'first')\n",
    "gu4.edge('blues', 'first')\n",
    "gu4.edge('first', 'second')\n",
    "gu4.edge('reds', 'second')\n",
    "gu4.edge('blues', 'second')\n",
    "gu4.render('urn4_graph', format='png')\n",
    "gu4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, you can write down the mathematical expression $p(B_1|R_2,I)$.\n",
    "This does not mean that $R_2$ is causing $B_1$.\n",
    "What happens here is that observing $R_2$ changes your state of knowledge about $B_1$.\n",
    "This is an example of information flowing in the reverse order of a causal link and a quintessential example of the inverse problem.\n",
    "Let's solve it analytically:\n",
    "$$\n",
    "\\begin{split}\n",
    "p(B_1|R_2,I) &=& \\frac{p(B_1,R_2|I)}{p(R_2|I)}\\\\\n",
    "&=& \\frac{\\frac{4}{15}}{0.6}\\\\\n",
    "&\\approx& 0.44.\n",
    "\\end{split}\n",
    "$$\n",
    "This is greater than the probability of drawing a blue ball in the first place, $p(B_1|I) = 0.4$.\n",
    "Does this make sense?\n",
    "Yes it does!\n",
    "Here is how you should think:\n",
    "+ You draw a ball without seeing it and you put in a box.\n",
    "+ You draw the second ball and you see that it is a red one.\n",
    "+ This means that this particular red ball was not picked in the first draw.\n",
    "+ So, it is as if in the first draw you had one less red to worry about which increases the probability of a blue.\n",
    "+ So, it is as if you had 5 red balls and 4 blue balls giving you a probability of blue $\\frac{4}{9}\\approx 0.44$.\n",
    "\n",
    "This is amazing!\n",
    "It agrees perfectly with the prediction of the product rule.\n",
    "This was one of our desiderata (if you compute something in two different ways you should get the same result).\n",
    "You can rest assured that as soon as you use the product rule and the sum rule and logic, it is impossible to get the wrong answer.\n",
    "That is, if you can actually carry out the computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
